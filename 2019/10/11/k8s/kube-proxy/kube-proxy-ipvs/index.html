<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css">


  <meta name="keywords" content="k8s,kube-proxy,service,">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2">






<meta name="description" content="转载 1 kube-proxy介绍1.1 为什么需要kube-proxy我们知道容器的特点是快速创建、快速销毁，Kubernetes Pod和容器一样只具有临时的生命周期，一个Pod随时有可能被终止或者漂移，随着集群的状态变化而变化，一旦Pod变化，则该Pod提供的服务也就无法访问，如果直接访问Pod则无法实现服务的连续性和高可用性，因此显然不能使用Pod地址作为服务暴露端口。 解决这个问题的办法">
<meta name="keywords" content="k8s,kube-proxy,service">
<meta property="og:type" content="article">
<meta property="og:title" content="kube-proxy -- ipvs">
<meta property="og:url" content="http://liupeng0518.github.io/2019/10/11/k8s/kube-proxy/kube-proxy-ipvs/index.html">
<meta property="og:site_name" content="Life is short, you need Python">
<meta property="og:description" content="转载 1 kube-proxy介绍1.1 为什么需要kube-proxy我们知道容器的特点是快速创建、快速销毁，Kubernetes Pod和容器一样只具有临时的生命周期，一个Pod随时有可能被终止或者漂移，随着集群的状态变化而变化，一旦Pod变化，则该Pod提供的服务也就无法访问，如果直接访问Pod则无法实现服务的连续性和高可用性，因此显然不能使用Pod地址作为服务暴露端口。 解决这个问题的办法">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/tcpdump_2.png">
<meta property="og:image" content="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/iptables_ipvs.png">
<meta property="og:image" content="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/dmesg_log.png">
<meta property="og:image" content="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/tcpdump_5.png">
<meta property="og:image" content="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/tcpdump_1.png">
<meta property="og:image" content="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/tcpdump_3.png">
<meta property="og:image" content="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/ping_cluster_ip.png">
<meta property="og:image" content="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/iptables_log_1.png">
<meta property="og:updated_time" content="2021-06-14T17:19:06.984Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="kube-proxy -- ipvs">
<meta name="twitter:description" content="转载 1 kube-proxy介绍1.1 为什么需要kube-proxy我们知道容器的特点是快速创建、快速销毁，Kubernetes Pod和容器一样只具有临时的生命周期，一个Pod随时有可能被终止或者漂移，随着集群的状态变化而变化，一旦Pod变化，则该Pod提供的服务也就无法访问，如果直接访问Pod则无法实现服务的连续性和高可用性，因此显然不能使用Pod地址作为服务暴露端口。 解决这个问题的办法">
<meta name="twitter:image" content="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/tcpdump_2.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://liupeng0518.github.io/2019/10/11/k8s/kube-proxy/kube-proxy-ipvs/">





  <title>kube-proxy -- ipvs | Life is short, you need Python</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-155243829-1', 'auto');
  ga('send', 'pageview');
</script>











<link rel="alternate" href="/atom.xml" title="Life is short, you need Python" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Life is short, you need Python</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Peng Liu`s Blog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://liupeng0518.github.io/2019/10/11/k8s/kube-proxy/kube-proxy-ipvs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Peng Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/ingress.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life is short, you need Python">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">kube-proxy -- ipvs</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-11T09:47:19+00:00">
                2019-10-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/" itemprop="url" rel="index">
                    <span itemprop="name">k8s</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><a href="https://int32bit.me/2019/11/28/IPVS从入门到精通kube-proxy实现原理/" target="_blank" rel="noopener">转载</a></p>
<h2 id="1-kube-proxy介绍"><a href="#1-kube-proxy介绍" class="headerlink" title="1 kube-proxy介绍"></a>1 kube-proxy介绍</h2><h3 id="1-1-为什么需要kube-proxy"><a href="#1-1-为什么需要kube-proxy" class="headerlink" title="1.1 为什么需要kube-proxy"></a>1.1 为什么需要kube-proxy</h3><p>我们知道容器的特点是快速创建、快速销毁，Kubernetes Pod和容器一样只具有临时的生命周期，一个Pod随时有可能被终止或者漂移，随着集群的状态变化而变化，一旦Pod变化，则该Pod提供的服务也就无法访问，如果直接访问Pod则无法实现服务的连续性和高可用性，因此显然不能使用Pod地址作为服务暴露端口。</p>
<p>解决这个问题的办法和传统数据中心解决无状态服务高可用的思路完全一样，通过负载均衡和VIP实现后端真实服务的自动转发、故障转移。</p>
<p>这个负载均衡在Kubernetes中称为Service，VIP即Service ClusterIP，因此可以认为Kubernetes的Service就是一个四层负载均衡，Kubernetes对应的还有七层负载均衡Ingress，本文仅介绍Kubernetes Service。</p>
<p>这个Service就是由kube-proxy实现的，ClusterIP不会因为Podz状态改变而变，需要注意的是VIP即ClusterIP是个假的IP，这个IP在整个集群中根本不存在，当然也就无法通过IP协议栈无法路由，底层underlay设备更无法感知这个IP的存在，因此ClusterIP只能是单主机(Host Only）作用域可见，这个IP在其他节点以及集群外均无法访问。</p>
<p>Kubernetes为了实现在集群所有的节点都能够访问Service，kube-proxy默认会在所有的Node节点都创建这个VIP并且实现负载，所以在部署Kubernetes后发现kube-proxy是一个DaemonSet。</p>
<p>而Service负载之所以能够在Node节点上实现是因为无论Kubernetes使用哪个网络模型，均需要保证满足如下三个条件：</p>
<ol>
<li>容器之间要求不需要任何NAT能直接通信；</li>
<li>容器与Node之间要求不需要任何NAT能直接通信；</li>
<li>容器看到自身的IP和外面看到它的IP必须是一样的，即不存在IP转化的问题。</li>
</ol>
<p>至少第2点是必须满足的，有了如上几个假设，Kubernetes Service才能在Node上实现，否则Node不通Pod IP也就实现不了了。</p>
<p>有人说既然kube-proxy是四层负载均衡，那kube-proxy应该可以使用haproxy、nginx等作为负载后端啊？</p>
<p>事实上确实没有问题，不过唯一需要考虑的就是性能问题，如上这些负载均衡功能都强大，但毕竟还是基于用户态转发或者反向代理实现的，性能必然不如在内核态直接转发处理好。</p>
<p>因此kube-proxy默认会优先选择基于内核态的负载作为后端实现机制，目前kube-proxy默认是通过iptables实现负载的，在此之前还有一种称为userspace模式，其实也是基于iptables实现，可以认为当前的iptables模式是对之前userspace模式的优化。</p>
<p>本节接下来将详细介绍kube-proxy iptables模式的实现原理。</p>
<h3 id="1-2-kube-proxy-iptables模式实现原理"><a href="#1-2-kube-proxy-iptables模式实现原理" class="headerlink" title="1.2 kube-proxy iptables模式实现原理"></a>1.2 kube-proxy iptables模式实现原理</h3><h4 id="1-2-1-ClusterIP"><a href="#1-2-1-ClusterIP" class="headerlink" title="1.2.1 ClusterIP"></a>1.2.1 ClusterIP</h4><p>首先创建了一个ClusterIP类型的Service:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get svc -l owner=int32bit</span><br><span class="line">NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">kubernetes-bootcamp-v1   ClusterIP   10.106.224.41   &lt;none&gt;        8080/TCP   163m</span><br></pre></td></tr></table></figure>
<p>其中ClusterIP为10.106.224.41，我们可以验证这个IP在本地是不存在的:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-172:~# ping -c 2 -w 2 10.106.224.41</span><br><span class="line">PING 10.106.224.41 (10.106.224.41) 56(84) bytes of data.</span><br><span class="line"></span><br><span class="line">--- 10.106.224.41 ping statistics ---</span><br><span class="line">2 packets transmitted, 0 received, 100% packet loss, time 1025ms</span><br><span class="line"></span><br><span class="line">root@ip-192-168-193-172:~# ip a | grep 10.106.224.41</span><br><span class="line">root@ip-192-168-193-172:~#</span><br></pre></td></tr></table></figure>
<p>所以<strong>不要尝试去ping ClusterIP，它不可能通的</strong>。</p>
<p>此时在Node节点192.168.193.172上访问该Service服务，首先流量到达的是OUTPUT链，这里我们只关心nat表的OUTPUT链：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># iptables-save -t nat | grep -- &apos;-A OUTPUT&apos;</span><br><span class="line">-A OUTPUT -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES</span><br></pre></td></tr></table></figure>
<p>该链跳转到<code>KUBE-SERVICES</code>子链中:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># iptables-save -t nat | grep -- &apos;-A KUBE-SERVICES&apos;</span><br><span class="line">...</span><br><span class="line">-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.106.224.41/32 -p tcp -m comment --comment &quot;default/kubernetes-bootcamp-v1: cluster IP&quot; -m tcp --dport 8080 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SERVICES -d 10.106.224.41/32 -p tcp -m comment --comment &quot;default/kubernetes-bootcamp-v1: cluster IP&quot; -m tcp --dport 8080 -j KUBE-SVC-RPP7DHNHMGOIIFDC</span><br></pre></td></tr></table></figure>
<p>我们发现与之相关的有两条规则：</p>
<ul>
<li>第一条负责打标记MARK<code>0x4000/0x4000</code>，后面会用到这个标记。</li>
<li>第二条规则跳到<code>KUBE-SVC-RPP7DHNHMGOIIFDC</code>子链。</li>
</ul>
<p>其中<code>KUBE-SVC-RPP7DHNHMGOIIFDC</code>子链规则如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># iptables-save -t nat | grep -- &apos;-A KUBE-SVC-RPP7DHNHMGOIIFDC&apos;</span><br><span class="line">-A KUBE-SVC-RPP7DHNHMGOIIFDC -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-FTIQ6MSD3LWO5HZX</span><br><span class="line">-A KUBE-SVC-RPP7DHNHMGOIIFDC -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-SQBK6CVV7ZCKBTVI</span><br><span class="line">-A KUBE-SVC-RPP7DHNHMGOIIFDC -j KUBE-SEP-IAZPHGLZVO2SWOVD</span><br></pre></td></tr></table></figure>
<p>这几条规则看起来复杂，其实实现的功能很简单:</p>
<ul>
<li>1/3的概率跳到子链<code>KUBE-SEP-FTIQ6MSD3LWO5HZX</code>,</li>
<li>剩下概率的1/2，(1 - 1/3) * 1/2 == 1/3，即1/3的概率跳到子链<code>KUBE-SEP-SQBK6CVV7ZCKBTVI</code>，</li>
<li>剩下1/3的概率跳到<code>KUBE-SEP-IAZPHGLZVO2SWOVD</code>。</li>
</ul>
<p>我们查看其中一个子链<code>KUBE-SEP-FTIQ6MSD3LWO5HZX</code>规则:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># iptables-save -t nat | grep -- &apos;-A KUBE-SEP-FTIQ6MSD3LWO5HZX&apos;</span><br><span class="line">...</span><br><span class="line">-A KUBE-SEP-FTIQ6MSD3LWO5HZX -p tcp -m tcp -j DNAT --to-destination 10.244.1.2:8080</span><br></pre></td></tr></table></figure>
<p>可见这条规则的目的是做了一次DNAT，DNAT目标为其中一个Endpoint，即Pod服务。</p>
<p>由此可见子链<code>KUBE-SVC-RPP7DHNHMGOIIFDC</code>的功能就是按照概率均等的原则DNAT到其中一个Endpoint IP，即Pod IP，假设为10.244.1.2，</p>
<p>此时相当于:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">192.168.193.172:xxxx -&gt; 10.106.224.41:8080</span><br><span class="line">                      |</span><br><span class="line">                      |  DNAT</span><br><span class="line">                      V</span><br><span class="line">192.168.193.172:xxxX -&gt; 10.244.1.2:8080</span><br></pre></td></tr></table></figure>
<p>接着来到POSTROUTING链:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># iptables-save -t nat | grep -- &apos;-A POSTROUTING&apos;</span><br><span class="line">-A POSTROUTING -m comment --comment &quot;kubernetes postrouting rules&quot; -j KUBE-POSTROUTING</span><br><span class="line"># iptables-save -t nat | grep -- &apos;-A KUBE-POSTROUTING&apos;</span><br><span class="line">-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -m mark --mark 0x4000/0x4000 -j MASQUERADE</span><br></pre></td></tr></table></figure>
<p>这两条规则只做一件事就是只要标记了<code>0x4000/0x4000</code>的包就一律做MASQUERADE（SNAT)，由于10.244.1.2默认是从flannel.1转发出去的，因此会把源IP改为flannel.1的IP<code>10.244.0.0</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">192.168.193.172:xxxx -&gt; 10.106.224.41:8080</span><br><span class="line">                      |</span><br><span class="line">                      |  DNAT</span><br><span class="line">                      V</span><br><span class="line">192.168.193.172:xxxx -&gt; 10.244.1.2:8080</span><br><span class="line">                      |</span><br><span class="line">                      |  SNAT</span><br><span class="line">                      V</span><br><span class="line">10.244.0.0:xxxx -&gt; 10.244.1.2:8080</span><br></pre></td></tr></table></figure>
<p>剩下的就是常规的走Vxlan隧道转发流程了，这里不再赘述，感兴趣的可以参考我之前的文章<a href="https://int32bit.me/2019/09/02/聊聊几种主流Docker网络的实现原理/" target="_blank" rel="noopener">浅聊几种主流Docker网络的实现原理</a>。</p>
<h4 id="1-2-2-NodePort"><a href="#1-2-2-NodePort" class="headerlink" title="1.2.2 NodePort"></a>1.2.2 NodePort</h4><p>接下来研究下NodePort过程，首先创建如下Service:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get svc -l owner=int32bit</span><br><span class="line">NAME                     TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kubernetes-bootcamp-v1   NodePort   10.106.224.41   &lt;none&gt;        8080:30419/TCP   3h30m</span><br></pre></td></tr></table></figure>
<p>其中Service的NodePort端口为30419。</p>
<p>假设有一个外部IP 192.168.193.197，通过<code>192.168.193.172:30419</code>访问服务。</p>
<p>首先到达PREROUTING链:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># iptables-save -t nat | grep -- &apos;-A PREROUTING&apos;</span><br><span class="line">-A PREROUTING -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES</span><br><span class="line"># iptables-save -t nat | grep -- &apos;-A KUBE-SERVICES&apos;</span><br><span class="line">...</span><br><span class="line">-A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS</span><br></pre></td></tr></table></figure>
<p>PREROUTING的规则非常简单，凡是发给自己的包，则交给子链<code>KUBE-NODEPORTS</code>处理。注意前面省略了判断ClusterIP的部分规则。</p>
<p><code>KUBE-NODEPORTS</code>规则如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># iptables-save -t nat | grep -- &apos;-A KUBE-NODEPORTS&apos;</span><br><span class="line">-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/kubernetes-bootcamp-v1:&quot; -m tcp --dport 30419 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/kubernetes-bootcamp-v1:&quot; -m tcp --dport 30419 -j KUBE-SVC-RPP7DHNHMGOIIFDC</span><br></pre></td></tr></table></figure>
<p>这个规则首先给包打上标记<code>0x4000/0x4000</code>，然后交给子链<code>KUBE-SVC-RPP7DHNHMGOIIFDC</code>处理，<code>KUBE-SVC-RPP7DHNHMGOIIFDC</code>刚刚已经见面过了，其功能就是按照概率均等的原则DNAT到其中一个Endpoint IP，即Pod IP，假设为10.244.1.2。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">192.168.193.197:xxxx -&gt; 192.168.193.172:30419</span><br><span class="line">                      |</span><br><span class="line">                      |  DNAT</span><br><span class="line">                      V</span><br><span class="line">192.168.193.197:xxxx -&gt; 10.244.1.2:8080</span><br></pre></td></tr></table></figure>
<p>此时发现10.244.1.2不是自己的IP，于是经过路由判断目标为10.244.1.2需要从flannel.1发出去。</p>
<p>接着到了<code>FORWARD</code>链，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># iptables-save -t filter | grep -- &apos;-A FORWARD&apos;</span><br><span class="line">-A FORWARD -m comment --comment &quot;kubernetes forwarding rules&quot; -j KUBE-FORWARD</span><br><span class="line"># iptables-save -t filter | grep -- &apos;-A KUBE-FORWARD&apos;</span><br><span class="line">-A KUBE-FORWARD -m conntrack --ctstate INVALID -j DROP</span><br><span class="line">-A KUBE-FORWARD -m comment --comment &quot;kubernetes forwarding rules&quot; -m mark --mark 0x4000/0x4000 -j ACCEPT</span><br></pre></td></tr></table></figure>
<p>FORWARD表在这里只是判断下，只允许打了标记<code>0x4000/0x4000</code>的包才允许转发。</p>
<p>最后来到<code>POSTROUTING</code>链，这里和ClusterIP就完全一样了，在<code>KUBE-POSTROUTING</code>中做一次<code>MASQUERADE</code>(SNAT)，最后结果:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">192.168.193.197:xxxx -&gt; 192.168.193.172:30419</span><br><span class="line">                      |</span><br><span class="line">                      |  DNAT</span><br><span class="line">                      V</span><br><span class="line">192.168.193.197:xxxx -&gt; 10.244.1.2:8080</span><br><span class="line">                      |</span><br><span class="line">                      |  SNAT</span><br><span class="line">                      V</span><br><span class="line">10.244.0.0:xxxx -&gt; 10.244.1.2:8080</span><br></pre></td></tr></table></figure>
<h3 id="1-3-kube-proxy使用iptables存在的问题"><a href="#1-3-kube-proxy使用iptables存在的问题" class="headerlink" title="1.3 kube-proxy使用iptables存在的问题"></a>1.3 kube-proxy使用iptables存在的问题</h3><p>我们发现基于iptables模式的kube-proxy ClusterIP和NodePort都是基于iptables规则实现的，我们至少发现存在如下几个问题：</p>
<ul>
<li>iptables规则复杂零乱，真要出现什么问题，排查iptables规则必然得掉层皮。<code>LOG + TRACE</code> 大法也不好使。</li>
<li>iptables规则多了之后性能下降，这是因为iptables规则是基于链表实现，查找复杂度为O(n)，当规模非常大时，查找和处理的开销就特别大。据<a href="https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/" target="_blank" rel="noopener">官方说法</a>，当节点到达5000个时，假设有2000个NodePort Service，每个Service有10个Pod，那么在每个Node节点中至少有20000条规则，内核根本支撑不住，iptables将成为最主要的性能瓶颈。</li>
<li>iptables主要是专门用来做主机防火墙的，而不是专长做负载均衡的。虽然通过iptables的<code>statistic</code>模块以及DNAT能够实现最简单的只支持概率轮询的负载均衡，但是往往我们还需要更多更灵活的算法，比如基于最少连接算法、源地址HASH算法等。而同样基于netfilter的ipvs却是专门做负载均衡的，配置简单，基于散列查找O(1)复杂度性能好，支持数十种调度算法。因此显然ipvs比iptables更适合做kube-proxy的后端，毕竟专业的人做专业的事，物尽其美。</li>
</ul>
<p>本文接下来将介绍kube-proxy的ipvs实现，由于本人之前也是对ipvs很陌生，没有用过，专门学习了下ipvs，因此在第二章简易介绍了下ipvs，如果已经很熟悉ipvs了，可以直接跳过，这一章和Kubernetes几乎没有任何关系。</p>
<p>另外由于本人对ipvs也是初学，水平有限，难免出错，欢迎指正！</p>
<h2 id="2-IPVS-简易入门"><a href="#2-IPVS-简易入门" class="headerlink" title="2 IPVS 简易入门"></a>2 IPVS 简易入门</h2><h3 id="2-1-IPVS简介"><a href="#2-1-IPVS简介" class="headerlink" title="2.1 IPVS简介"></a>2.1 IPVS简介</h3><p>我们接触比较多的是应用层负载均衡，比如haproxy、nginx、F5等，这些负载均衡工作在用户态，因此会有对应的进程和监听socket，一般能同时支持4层负载和7层负载，使用起来也比较方便。</p>
<p>LVS是国内章文嵩博士开发并贡献给社区的（<a href="http://jm.taobao.org/2016/06/02/zhangwensong-and-load-balance/" target="_blank" rel="noopener">章文嵩博士和他背后的负载均衡帝国 </a>)，主要由ipvs和ipvsadm组成，ipvs是工作在内核态的4层负载均衡，和iptables一样都是基于内核底层netfilter实现，netfilter主要通过各个链的钩子实现包处理和转发。ipvsadm和ipvs的关系，就好比netfilter和iptables的关系，它运行在用户态，提供简单的CLI接口进行ipvs配置。</p>
<p>由于ipvs工作在内核态，直接基于内核处理包转发，所以最大的特点就是性能非常好。又由于它工作在4层，因此不会处理应用层数据，经常有人问ipvs能不能做SSL证书卸载、或者修改HTTP头部数据，显然这些都不可能做的。</p>
<p>我们知道应用层负载均衡大多数都是基于反向代理实现负载的，工作在应用层，当用户的包到达负载均衡监听器listening后，基于一定的算法从后端服务列表中选择其中一个后端服务进行转发。当然中间可能还会有一些额外操作，最常见的如SSL证书卸载。</p>
<p>而ipvs工作在内核态，只处理四层协议，因此只能基于路由或者NAT进行数据转发，可以把ipvs当作一个特殊的路由器网关，这个网关可以根据一定的算法自动选择下一跳，或者把ipvs当作一个多重DNAT，按照一定的算法把ip包的目标地址DNAT到其中真实服务的目标IP。针对如上两种情况分别对应ipvs的两种模式–网关模式和NAT模式，另外ipip模式则是对网关模式的扩展，本文下面会针对这几种模式的实现原理进行详细介绍。</p>
<h3 id="2-2-IPVS用法"><a href="#2-2-IPVS用法" class="headerlink" title="2.2 IPVS用法"></a>2.2 IPVS用法</h3><p>ipvsadm命令行用法和iptables命令行用法非常相似，毕竟是兄弟，比如<code>-L</code>列举，<code>-A</code>添加，<code>-D</code>删除。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -A -t 192.168.193.172:32016 -s rr</span><br></pre></td></tr></table></figure>
<p>但是其实ipvsadm相对iptables命令简直太简单了，因为没有像iptables那样存在各种table，table嵌套各种链，链里串着一堆规则，ipvsadm就只有两个核心实体，分别为service和server，service就是一个负载均衡实例，而server就是后端member,ipvs术语中叫做real server，简称RS。</p>
<p>如下命令创建一个service实例<code>172.17.0.1:32016</code>，<code>-t</code>指定监听的为<code>TCP</code>端口，<code>-s</code>指定算法为轮询算法rr(Round Robin)，ipvs支持简单轮询(rr)、加权轮询(wrr)、最少连接(lc)、源地址或者目标地址散列(sh、dh)等10种调度算法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -A -t 172.17.0.1:32016 -s rr</span><br></pre></td></tr></table></figure>
<p>然后把10.244.1.2:8080、10.244.1.3:8080、10.244.3.2:8080添加到service后端member中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -a -t 172.17.0.1:32016 -r 10.244.1.2:8080 -m -w 1</span><br><span class="line">ipvsadm -a -t 172.17.0.1:32016 -r 10.244.1.3:8080 -m -w 1</span><br><span class="line">ipvsadm -a -t 172.17.0.1:32016 -r 10.244.3.2:8080 -m -w 1</span><br></pre></td></tr></table></figure>
<p>其中<code>-t</code>指定service实例，<code>-r</code>指定server地址，<code>-w</code>指定权值，<code>-m</code>即前面说的转发模式，其中<code>-m</code>表示为<code>masquerading</code>，即NAT模式，<code>-g</code>为<code>gatewaying</code>，即直连路由模式，<code>-i</code>为<code>ipip</code>,ji即IPIP隧道模式。</p>
<p>与iptables-save、iptables-restore对应的工具ipvs也有ipvsadm-save、ipvsadm-restore。</p>
<h3 id="2-3-NAT-network-access-translation-模式"><a href="#2-3-NAT-network-access-translation-模式" class="headerlink" title="2.3 NAT(network access translation)模式"></a>2.3 NAT(network access translation)模式</h3><p>NAT模式由字面意思理解就是通过NAT实现的，但究竟是如何NAT转发的，我们通过实验环境验证下。</p>
<p>现环境中LB节点IP为192.168.193.197，三个RS节点如下:</p>
<ul>
<li>192.168.193.172:30620</li>
<li>192.168.193.194:30620</li>
<li>192.168.193.226:30620</li>
</ul>
<p>为了模拟LB节点IP和RS不在同一个网络的情况，在LB节点中添加一个虚拟IP地址:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip addr add 10.222.0.1/24 dev ens5</span><br></pre></td></tr></table></figure>
<p>创建负载均衡Service并把RS添加到Service中:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -A -t 10.222.0.1:8080 -s rr</span><br><span class="line">ipvsadm -a -t 10.222.0.1:8080 -r 192.168.193.194:30620 -m</span><br><span class="line">ipvsadm -a -t 10.222.0.1:8080 -r 192.168.193.226:30620 -m</span><br><span class="line">ipvsadm -a -t 10.222.0.1:8080 -r 192.168.193.172:30620 -m</span><br></pre></td></tr></table></figure>
<p>这里需要注意的是，和应用层负载均衡如haproxy、nginx不一样的是，haproxy、nginx进程是运行在用户态，因此会创建socket，本地会监听端口，而<strong>ipvs的负载是直接运行在内核态的，因此不会出现监听端口</strong>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-197:/var/log# netstat -lnpt</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      674/systemd-resolve</span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      950/sshd</span><br><span class="line">tcp6       0      0 :::22                   :::*                    LISTEN      950/sshd</span><br></pre></td></tr></table></figure>
<p><strong>可见并没有监听10.222.0.1:8080 Socket</strong>。</p>
<p>Client节点IP为192.168.193.226，为了和LB节点的虚拟IP 10.222.0.1通，我们手动添加静态路由如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip r add 10.222.0.1 via 192.168.193.197 dev ens5</span><br></pre></td></tr></table></figure>
<p>此时Client节点能够ping通LB节点VIP:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~# ping -c 2 -w 2 10.222.0.1</span><br><span class="line">PING 10.222.0.1 (10.222.0.1) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.222.0.1: icmp_seq=1 ttl=64 time=0.345 ms</span><br><span class="line">64 bytes from 10.222.0.1: icmp_seq=2 ttl=64 time=0.249 ms</span><br><span class="line"></span><br><span class="line">--- 10.222.0.1 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 1022ms</span><br><span class="line">rtt min/avg/max/mdev = 0.249/0.297/0.345/0.048 ms</span><br></pre></td></tr></table></figure>
<p>可见Client节点到VIP的链路没有问题，那是否能够访问我们的Service呢？</p>
<p>我们验证下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~# curl -m 2 --retry 1 -sSL 10.222.0.1:8080</span><br><span class="line">curl: (28) Connection timed out after 2001 milliseconds</span><br></pre></td></tr></table></figure>
<p>非常意外的结果是并不通。</p>
<p>在RS节点抓包如下:</p>
<p><img src="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/tcpdump_2.png" alt="tcpdump 2"></p>
<p>我们发现数据包的源IP为Client IP，目标IP为RS IP，换句话说，LB节点IPVS只做了DNAT，把目标IP改成RS IP了，而没有修改源IP。此时虽然RS和Client在同一个子网，链路连通性没有问题，但是由于Client节点发出去的包的目标IP和收到的包源IP不一致，因此会被直接丢弃，相当于给张三发信，李四回的信，显然不受信任。</p>
<p>既然IPVS没有给我们做SNAT，那自然想到的是我们手动做SNAT，在LB节点添加如下iptables规则：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A POSTROUTING -m ipvs  --vaddr 10.222.0.1 --vport 8080 -j LOG --log-prefix &apos;[int32bit ipvs]&apos;</span><br><span class="line">iptables -t nat -A POSTROUTING -m ipvs  --vaddr 10.222.0.1 --vport 8080 -j MASQUERADE</span><br></pre></td></tr></table></figure>
<p>再次检查Service是否可以访问:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~# curl -m 2 --retry 1 -sSL 10.222.0.1:8080</span><br><span class="line">curl: (28) Connection timed out after 2001 milliseconds</span><br></pre></td></tr></table></figure>
<p>服务依然不通。并且在LB节点的iptables日志为空:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-197:~# cat /var/log/syslog | grep &apos;int32bit ipvs&apos;</span><br><span class="line">root@ip-192-168-193-197:~#</span><br></pre></td></tr></table></figure>
<p>也就是说，ipvs的包根本不会经过iptables nat表POSTROUTING链？</p>
<p>那mangle表呢？我们打开LOG查看下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t mangle -A POSTROUTING -m ipvs --vaddr 10.222.0.1 --vport 8080 -j LOG --log-prefix &quot;[int32bit ipvs]&quot;</span><br></pre></td></tr></table></figure>
<p>此时查看日志如下：</p>
<p><img src="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/iptables_ipvs.png" alt="iptables ipvs"></p>
<p>我们发现在mangle表中可以看到DNAT后的包。</p>
<p>只是可惜mangle表的POSTROUTING并不支持NAT功能:</p>
<p><img src="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/dmesg_log.png" alt="dmesg log"></p>
<p>对比Kubernetes配置发现需要设置如下系统参数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl net.ipv4.vs.conntrack=1</span><br></pre></td></tr></table></figure>
<p>再次验证：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~# curl -i 10.222.0.1:8080</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Content-Type: text/plain</span><br><span class="line">Date: Wed, 27 Nov 2019 15:28:06 GMT</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Transfer-Encoding: chunked</span><br><span class="line"></span><br><span class="line">Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-v1-c5ccf9784-g9bkx | v=1</span><br></pre></td></tr></table></figure>
<p>终于通了，查看RS抓包：</p>
<p><img src="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/tcpdump_5.png" alt="img"></p>
<p>如期望，修改了源IP为LB IP。</p>
<p>原来需要配置<code>net.ipv4.vs.conntrack=1</code>参数，这个问题折腾了一个晚上，不得不说目前ipvs的文档都太老了。</p>
<p>前面是通过手动iptables实现SNAT的，性能可能会有损耗，于是如下开源项目通过修改lvs直接做SNAT:</p>
<ul>
<li>小米运维部在LVS的FULLNAT基础上，增加了SNAT网关功能，参考<a href="https://github.com/xiaomi-sa/dsnat" target="_blank" rel="noopener">xiaomi-sa/dsnat</a></li>
<li><a href="https://github.com/jlijian3/lvs-snat" target="_blank" rel="noopener">lvs-snat</a></li>
</ul>
<p>除了SNAT的办法，是否还有其他办法呢？想想我们最初的问题，Client节点发出去的包的目标IP和收到的包源IP不一致导致包被丢弃，那解决问题的办法就是把包重新引到LB节点上，只需要在所有的RS节点增加如下路由即可:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip r add 192.168.193.226 via 192.168.193.197 dev ens5</span><br></pre></td></tr></table></figure>
<p>此时我们再次检查我们的Service是否可连接:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~# curl -i -m 2 --retry 1 -sSL 10.222.0.1:8080</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Content-Type: text/plain</span><br><span class="line">Date: Wed, 27 Nov 2019 03:21:47 GMT</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Transfer-Encoding: chunked</span><br><span class="line"></span><br><span class="line">Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-v1-c5ccf9784-4v9z4 | v=1</span><br></pre></td></tr></table></figure>
<p>结果没有问题。</p>
<p>不过我们是通过手动添加Client IP到所有RS的明细路由实现的，如果Client不固定，这种方案仍然不太可行，所以通常做法是干脆把所有RS默认路由指向LB节点，即把LB节点当作所有RS的默认网关。</p>
<p>由此可知，用户通过LB地址访问服务，LB节点IPVS会把用户的目标IP由LB IP改为RS IP，源IP不变，包不经过iptables的OUTPUT直接到达POSTROUTING转发出去，包回来的时候也必须先到LB节点，LB节点把目标IP再改成用户的源IP，最后转发给用户。</p>
<p>显然这种模式来回都需要经过LB节点，因此又称为双臂模式。</p>
<h3 id="2-4-网关-Gatewaying-模式"><a href="#2-4-网关-Gatewaying-模式" class="headerlink" title="2.4 网关(Gatewaying)模式"></a>2.4 网关(Gatewaying)模式</h3><p>网关模式（Gatewaying）又称为直连路由模式（Direct Routing）、透传模式，<strong>所谓透传即LB节点不会修改数据包的源IP、端口以及目标IP、端口</strong>，LB节点做的仅仅是路由转发出去，可以把LB节点看作一个特殊的路由器网关，而RS节点则是网关的下一跳，这就相当于对于同一个目标地址，会有多个下一跳，这个路由器网关的特殊之处在于能够根据一定的算法选择其中一个RS作为下一跳，达到负载均衡和冗余的效果。</p>
<p>既然是通过直连路由的方式转发，那显然LB节点必须与所有的RS节点在同一个子网，不能跨子网，否则路由不可达。换句话说，<strong>这种模式只支持内部负载均衡(Internal LoadBalancer)</strong>。</p>
<p>另外如前面所述，LB节点不会修改源端口和目标端口，因此这种模式也无法支持端口映射，换句话说<strong>LB节点监听的端口和所有RS节点监听的端口必须一致</strong>。</p>
<p>现在假设有LB节点IP为<code>192.168.193.197</code>，有三个RS节点如下:</p>
<ul>
<li>192.168.193.172:30620</li>
<li>192.168.193.194:30620</li>
<li>192.168.193.226:30620</li>
</ul>
<p>创建负载均衡Service并把RS添加到Service中:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -A -t 192.168.193.197:30620 -s rr</span><br><span class="line">ipvsadm -a -t 192.168.193.197:30620 -r 192.168.193.194:30620 -g</span><br><span class="line">ipvsadm -a -t 192.168.193.197:30620 -r 192.168.193.226:30620 -g</span><br><span class="line">ipvsadm -a -t 192.168.193.197:30620 -r 192.168.193.172:30620 -g</span><br></pre></td></tr></table></figure>
<p>注意到我们的Service监听的端口30620和RS的端口是一样的，并且通过<code>-g</code>参数指定为直连路由模式(网关模式)。</p>
<p>Client节点IP为192.168.193.226，我们验证Service是否可连接：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~# curl -m 5 -sSL 192.168.193.197:30620</span><br><span class="line">curl: (28) Connection timed out after 5001 milliseconds</span><br></pre></td></tr></table></figure>
<p>我们发现并不通，在其中一个RS节点192.168.193.172上抓包:</p>
<p><img src="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/tcpdump_1.png" alt="tcpdump"></p>
<p>正如前面所说，LB是通过路由转发的，根据路由的原理，源MAC地址修改为LB的MAC地址，而目标MAC地址修改为RS MAC地址，相当于RS是LB的下一跳。</p>
<p>并且源IP和目标IP都不会修改。问题就来了，我们Client期望访问的是RS，但RS收到的目标IP却是LB的IP，发现这个目标IP并不是自己的IP，因此不会通过INPUT链转发到用户空间，这时要不直接丢弃这个包，要不根据路由再次转发到其他地方，总之两种情况都不是我们期望的结果。</p>
<p>那怎么办呢？为了让RS接收这个包，必须得让RS有这个目标IP才行。于是不妨在lo上添加个虚拟IP，IP地址伪装成LB IP 192.168.193.197:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig lo:0 192.168.193.197/32</span><br></pre></td></tr></table></figure>
<p>问题又来了，这就相当于有两个相同的IP，IP重复了怎么办？办法是隐藏这个虚拟网卡，不让它回复ARP，其他主机的neigh也就不可能知道有这么个网卡的存在了，参考<a href="http://kb.linuxvirtualserver.org/wiki/Using_arp_announce/arp_ignore_to_disable_ARP" target="_blank" rel="noopener">Using arp announce/arp ignore to disable ARP</a>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sysctl net.ipv4.conf.lo.arp_ignore=1</span><br><span class="line">sysctl net.ipv4.conf.lo.arp_announce=2</span><br></pre></td></tr></table></figure>
<p>此时再次从客户端curl:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~# curl -m 2 --retry 1 -sSL 192.168.193.197:30620</span><br><span class="line">Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-v1-c5ccf9784-4v9z4 | v=1</span><br></pre></td></tr></table></figure>
<p>终于通了。</p>
<p>我们从前面的抓包中知道，源IP为Client IP 192.168.193.226，因此直接回包给Client即可，不可能也不需要再回到LB节点了，即A-&gt;B,B-&gt;C，C-&gt;A，流量方向是三角形状的，因此这种模式又称为三角模式。</p>
<p>我们从原理中不难得出如下结论：</p>
<ul>
<li>Client、LB以及所有的RS必须在同一个子网。</li>
<li>LB节点直接通过路由转发，因此性能非常高。</li>
<li>不能做端口映射。</li>
</ul>
<h3 id="2-5-ipip隧道模式"><a href="#2-5-ipip隧道模式" class="headerlink" title="2.5 ipip隧道模式"></a>2.5 ipip隧道模式</h3><p>前面介绍了网关直连路由模式，要求所有的节点在同一个子网，而ipip隧道模式则主要解决这种限制，LB节点IP和RS可以不在同一个子网，此时需要通过ipip隧道进行传输。</p>
<p>现在假设有LB节点IP为<code>192.168.193.77/25</code>，在该节点上增加一个VIP地址:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip addr add 192.168.193.48/25 dev eth0</span><br></pre></td></tr></table></figure>
<p>有三个RS节点如下:</p>
<ul>
<li>192.168.193.172:30620</li>
<li>192.168.193.194:30620</li>
<li>192.168.193.226:30620</li>
</ul>
<p>如上三个RS节点子网掩码均为255.255.255.128，即25位子网，显然和VIP 192.168.193.48/25不在同一个子网。</p>
<p>创建负载均衡Service并把RS添加到Service中:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -A -t 192.168.193.48:30620 -s rr</span><br><span class="line">ipvsadm -a -t 192.168.193.48:30620 -r 192.168.193.194:30620 -i</span><br><span class="line">ipvsadm -a -t 192.168.193.48:30620 -r 192.168.193.226:30620 -i</span><br><span class="line">ipvsadm -a -t 192.168.193.48:30620 -r 192.168.193.172:30620 -i</span><br></pre></td></tr></table></figure>
<p>注意到我们的Service监听的端口30620和RS的端口是一样的，并且通过<code>-i</code>参数指定为ipip隧道模式。</p>
<p>在所有的RS节点上加载ipip模块以及添加VIP(和直连路由类型）:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">modprobe ipip</span><br><span class="line">ifconfig tunl0  192.168.193.48/32</span><br><span class="line">sysctl net.ipv4.conf.tunl0.arp_ignore=1</span><br><span class="line">sysctl net.ipv4.conf.tunl0.arp_announce=2</span><br></pre></td></tr></table></figure>
<p>Client节点IP为192.168.193.226/25，我们验证Service是否可连接：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~# curl -i -sSL 192.168.193.48:30620</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Content-Type: text/plain</span><br><span class="line">Date: Wed, 27 Nov 2019 07:05:40 GMT</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Transfer-Encoding: chunked</span><br><span class="line"></span><br><span class="line">Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-v1-c5ccf9784-dgn74 | v=1</span><br><span class="line">root@ip-192-168-193-226:~#</span><br></pre></td></tr></table></figure>
<p>Service可访问，我们在RS节点上抓包如下：</p>
<p><img src="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/tcpdump_3.png" alt="tcpdump"></p>
<p>我们发现和直连路由一样，源IP和目标IP没有修改。</p>
<p>所以IPIP模式和网关(Gatewaying)模式原理基本一样，唯一不同的是网关(Gatewaying)模式要求所有的RS节点和LB节点在同一个子网，而IPIP模式则可以支持跨子网的情况，为了解决跨子网通信问题，使用了ipip隧道进行数据传输。</p>
<h3 id="2-4-总结"><a href="#2-4-总结" class="headerlink" title="2.4 总结"></a>2.4 总结</h3><p>ipvs是一个内核态的四层负载均衡，支持NAT、Gateway以及IPIP隧道模式，Gateway模式性能最好，但LB和RS不能跨子网，IPIP性能次之，通过ipip隧道解决跨网段传输问题，因此能够支持跨子网。而NAT模式没有限制，这也是唯一一种支持端口映射的模式。</p>
<p>我们不难猜想，由于Kubernetes Service需要使用端口映射功能，因此kube-proxy必然只能使用ipvs的NAT模式。</p>
<h2 id="3-kube-proxy使用ipvs模式"><a href="#3-kube-proxy使用ipvs模式" class="headerlink" title="3 kube-proxy使用ipvs模式"></a>3 kube-proxy使用ipvs模式</h2><h3 id="3-1-配置kube-proxy使用ipvs模式"><a href="#3-1-配置kube-proxy使用ipvs模式" class="headerlink" title="3.1 配置kube-proxy使用ipvs模式"></a>3.1 配置kube-proxy使用ipvs模式</h3><p>使用kubeadm安装Kubernetes可参考文档<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md#cluster-created-by-kubeadm" target="_blank" rel="noopener">Cluster Created by Kubeadm</a>，不过这个文档的安装配置有问题<a href="https://github.com/kubernetes/kubeadm/issues/1182" target="_blank" rel="noopener">kubeadm #1182</a>，如下官方配置不生效:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    featureGates:</span><br><span class="line">      SupportIPVSProxyMode: true</span><br><span class="line">    mode: ipvs</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>需要修改为如下配置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: ipvs</span><br></pre></td></tr></table></figure>
<p>可以通过如下命令确认kube-proxy是否修改为ipvs:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get configmaps kube-proxy -n kube-system -o yaml | awk &apos;/mode/&#123;print $2&#125;&apos;</span><br><span class="line">ipvs</span><br></pre></td></tr></table></figure>
<h3 id="3-2-Service-ClusterIP原理"><a href="#3-2-Service-ClusterIP原理" class="headerlink" title="3.2 Service ClusterIP原理"></a>3.2 Service ClusterIP原理</h3><p>创建一个ClusterIP类似的Service如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get svc | grep kubernetes-bootcamp-v1</span><br><span class="line">kubernetes-bootcamp-v1   ClusterIP   10.96.54.11   &lt;none&gt;        8080/TCP   2m11s</span><br></pre></td></tr></table></figure>
<p>ClusterIP 10.96.54.11为我们查看ipvs配置如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># ipvsadm -S -n | grep 10.96.54.11</span><br><span class="line">-A -t 10.96.54.11:8080 -s rr</span><br><span class="line">-a -t 10.96.54.11:8080 -r 10.244.1.2:8080 -m -w 1</span><br><span class="line">-a -t 10.96.54.11:8080 -r 10.244.1.3:8080 -m -w 1</span><br><span class="line">-a -t 10.96.54.11:8080 -r 10.244.2.2:8080 -m -w 1</span><br></pre></td></tr></table></figure>
<p>可见ipvs的LB IP为ClusterIP，算法为rr，RS为Pod的IP。</p>
<p>另外我们发现使用的模式为NAT模式，这是显然的，因为除了NAT模式支持端口映射，其他两种均不支持端口映射，所以必须选择NAT模式。</p>
<p>由前面的理论知识，ipvs的VIP必须在本地存在，我们可以验证:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># ip addr show kube-ipvs0</span><br><span class="line">4: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default</span><br><span class="line">    link/ether 46:6b:9e:af:b0:60 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.96.0.1/32 brd 10.96.0.1 scope global kube-ipvs0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 10.96.0.10/32 brd 10.96.0.10 scope global kube-ipvs0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 10.96.54.11/32 brd 10.96.54.11 scope global kube-ipvs0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"># ethtool -i kube-ipvs0 | grep driver</span><br><span class="line">driver: dummy</span><br></pre></td></tr></table></figure>
<p>可见kube-proxy首先会创建一个dummy虚拟网卡kube-ipvs0，然后把所有的Service IP添加到kube-ipvs0中。</p>
<p>我们知道基于iptables的Service，ClusterIP是一个虚拟的IP，因此这个IP是ping不通的，但ipvs中这个IP是在每个节点上真实存在的，因此可以ping通:</p>
<p><img src="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/ping_cluster_ip.png" alt="ping cluster ip"></p>
<p>当然由于这个IP就是配置在本地虚拟网卡上，所以对诊断问题没有一点用处的。</p>
<p>我们接下来研究下ClusterIP如何传递的。</p>
<p>当我们通过如下命令连接服务时:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl 10.96.54.11:8080</span><br></pre></td></tr></table></figure>
<p>此时由于10.96.54.11就在本地，所以会以这个IP作为出口地址，即源IP和目标IP都是10.96.54.11，此时相当于:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.96.54.11:xxxx -&gt; 10.96.54.11:8080</span><br></pre></td></tr></table></figure>
<p>其中xxxx为随机端口。</p>
<p>然后经过ipvs，ipvs会从RS ip列中选择其中一个Pod ip作为目标IP，假设为10.244.2.2:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">10.96.54.11:xxxx -&gt; 10.96.54.11:8080</span><br><span class="line">                 |</span><br><span class="line">                 | IPVS</span><br><span class="line">                 v</span><br><span class="line">10.96.54.11:xxxx -&gt; 10.244.2.2:8080</span><br></pre></td></tr></table></figure>
<p>我们从iptables LOG可以验证:</p>
<p><img src="https://int32bit.me/img/posts/IPVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9Akube-proxy%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/iptables_log_1.png" alt="iptables log"></p>
<p>我们查看OUTPUT安全组规则如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-A OUTPUT -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES</span><br><span class="line">-A KUBE-SERVICES ! -s 10.244.0.0/16 -m comment --comment &quot;Kubernetes service cluster ip + port for masquerade purpose&quot; -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000</span><br></pre></td></tr></table></figure>
<p>其中ipsetj集合<code>KUBE-CLUSTER-IP</code>保存着所有的ClusterIP以及监听端口。</p>
<p>如上规则的意思就是除了Pod以外访问ClusterIP的包都打上<code>0x4000/0x4000</code>。</p>
<p>到了POSTROUTING链:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-A POSTROUTING -m comment --comment &quot;kubernetes postrouting rules&quot; -j KUBE-POSTROUTING</span><br><span class="line">-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -m mark --mark 0x4000/0x4000 -j MASQUERADE</span><br></pre></td></tr></table></figure>
<p>如上规则的意思就是只要匹配mark<code>0x4000/0x4000</code>的包都做SNAT，由于10.244.2.2是从flannel.1出去的，因此源ip会改成flannel.1的ip <code>10.244.0.0</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">10.96.54.11:xxxx -&gt; 10.96.54.11:8080</span><br><span class="line">                 |</span><br><span class="line">                 | IPVS</span><br><span class="line">                 v</span><br><span class="line">10.96.54.11:xxxx -&gt; 10.244.2.2:8080</span><br><span class="line">                 |</span><br><span class="line">                 | MASQUERADE</span><br><span class="line">                 v</span><br><span class="line">10.244.0.0:xxxx -&gt; 10.244.2.2:8080</span><br></pre></td></tr></table></figure>
<p>最后通过Vxlan 隧道发到Pod的Node上，转发给Pod的veth，回包通过路由到达源Node节点，源Node节点通过之前的MASQUERADE再把目标IP还原为10.96.54.11。</p>
<h3 id="3-3-NodeIP实现原理"><a href="#3-3-NodeIP实现原理" class="headerlink" title="3.3 NodeIP实现原理"></a>3.3 NodeIP实现原理</h3><p>查看Service如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-172:~# kubectl get svc</span><br><span class="line">NAME                     TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kubernetes               ClusterIP   10.96.0.1     &lt;none&gt;        443/TCP          30h</span><br><span class="line">kubernetes-bootcamp-v1   NodePort    10.96.54.11   &lt;none&gt;        8080:32016/TCP   8h</span><br></pre></td></tr></table></figure>
<p>Service kubernetes-bootcamp-v1的NodePort为32016。</p>
<p>现在假设集群外的一个IP 192.168.193.197访问192.168.193.172:32016:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.193.197:xxxx -&gt; 192.168.193.172:32016</span><br></pre></td></tr></table></figure>
<p>最先到达PREROUTING链:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-A PREROUTING -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES</span><br><span class="line">-A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT</span><br><span class="line">-A KUBE-NODE-PORT -p tcp -m comment --comment &quot;Kubernetes nodeport TCP port for masquerade purpose&quot; -m set --match-set KUBE-NODE-PORT-TCP dst -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000</span><br></pre></td></tr></table></figure>
<p>如上4条规则看起来复杂，其实就做一件事，如果目标地址为NodeIP，则把包标记<code>0x4000</code>，<code>0x4000</code>。</p>
<p>我们查看ipvs:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># ipvsadm -S -n | grep 32016</span><br><span class="line">-A -t 192.168.193.172:32016 -s rr</span><br><span class="line">-a -t 192.168.193.172:32016 -r 10.244.1.2:8080 -m -w 1</span><br><span class="line">-a -t 192.168.193.172:32016 -r 10.244.1.3:8080 -m -w 1</span><br><span class="line">-a -t 192.168.193.172:32016 -r 10.244.3.2:8080 -m -w 1</span><br></pre></td></tr></table></figure>
<p>我们发现和ClusterIP实现原理非常相似，ipvs Service的VIP为Node IP，端口为NodePort。ipvs会选择其中一个Pod IP作为DNAT目标，这里假设为10.244.3.2：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">192.168.193.197:xxxx -&gt; 192.168.193.172:32016</span><br><span class="line">                     |</span><br><span class="line">                     | DNAT</span><br><span class="line">                     v</span><br><span class="line">192.168.193.197:xxx  --&gt; 10.244.3.2:8080</span><br></pre></td></tr></table></figure>
<p>剩下的到了POSTROUTING链就和Service ClusterIP完全一样了，只要匹配<code>0x4000/0x4000</code>的包就会做SNAT。</p>
<h3 id="3-4-总结"><a href="#3-4-总结" class="headerlink" title="3.4 总结"></a>3.4 总结</h3><p>Kubernetes的ClusterIP和NodePort都是通过ipvs service实现的，Pod当作ipvs service的server，通过NAT MQSQ实现转发。</p>
<p>简单来说kube-proxy主要在所有的Node节点做如下三件事:</p>
<ol>
<li>如果没有dummy类型虚拟网卡，则创建一个，默认名称为<code>kube-ipvs0</code>;</li>
<li>把Kubernetes ClusterIP地址添加到<code>kube-ipvs0</code>，同时添加到ipset中。</li>
<li>创建ipvs service，ipvs service地址为ClusterIP以及Cluster Port，ipvs server为所有的Endpoint地址，即Pod IP及端口。</li>
</ol>
<p>使用ipvs作为kube-proxy后端，不仅提高了转发性能，结合ipset还使iptables规则变得更“干净”清楚，从此再也不怕iptables。</p>
<p>更多关于kube-proxy ipvs参考<a href="https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/" target="_blank" rel="noopener">IPVS-Based In-Cluster Load Balancing Deep Dive</a>.</p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p>本文首先介绍了kube-proxy的功能以及kube-proxy基于iptables的实现原理，然后简单介绍了ipvs，了解了ipvs支持的三种转发模式，最后介绍了kube-proxy基于ipvs的实现原理。</p>
<p>ipvs是专门设计用来做内核态四层负载均衡的，由于使用了hash表的数据结构，因此相比iptables来说性能会更好。基于ipvs实现Service转发，Kubernetes几乎能够具备无限的水平扩展能力。随着Kubernetes的部署规模越来越大，应用越来越广泛，ipvs必然会取代iptables成为Kubernetes Service的默认实现后端。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/k8s/" rel="tag"># k8s</a>
          
            <a href="/tags/kube-proxy/" rel="tag"># kube-proxy</a>
          
            <a href="/tags/service/" rel="tag"># service</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/10/09/docker/runtimes/Container Runtimes Part 4/" rel="next" title="容器运行时 4 - Kubernetes Container Runtimes & CRI">
                <i class="fa fa-chevron-left"></i> 容器运行时 4 - Kubernetes Container Runtimes & CRI
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/10/11/k8s/Network/how to inspect kubernetes networking/" rel="prev" title="how to inspect kubernetes networking">
                how to inspect kubernetes networking <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/ingress.png" alt="Peng Liu">
          <p class="site-author-name" itemprop="name">Peng Liu</p>
           
              <p class="site-description motion-element" itemprop="description">搬砖爱好者</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">136</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">21</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">88</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/liupeng0518" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/liupeng0518" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                    
                      Twitter
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/youngmario" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      微博
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/mario007" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      知乎
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-kube-proxy介绍"><span class="nav-number">1.</span> <span class="nav-text">1 kube-proxy介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-为什么需要kube-proxy"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 为什么需要kube-proxy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-kube-proxy-iptables模式实现原理"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 kube-proxy iptables模式实现原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-ClusterIP"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.2.1 ClusterIP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-2-NodePort"><span class="nav-number">1.2.2.</span> <span class="nav-text">1.2.2 NodePort</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-kube-proxy使用iptables存在的问题"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 kube-proxy使用iptables存在的问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-IPVS-简易入门"><span class="nav-number">2.</span> <span class="nav-text">2 IPVS 简易入门</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-IPVS简介"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 IPVS简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-IPVS用法"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 IPVS用法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-NAT-network-access-translation-模式"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 NAT(network access translation)模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-网关-Gatewaying-模式"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 网关(Gatewaying)模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-ipip隧道模式"><span class="nav-number">2.5.</span> <span class="nav-text">2.5 ipip隧道模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-总结"><span class="nav-number">2.6.</span> <span class="nav-text">2.4 总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-kube-proxy使用ipvs模式"><span class="nav-number">3.</span> <span class="nav-text">3 kube-proxy使用ipvs模式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-配置kube-proxy使用ipvs模式"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 配置kube-proxy使用ipvs模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Service-ClusterIP原理"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 Service ClusterIP原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-NodeIP实现原理"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 NodeIP实现原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-总结"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-总结"><span class="nav-number">4.</span> <span class="nav-text">4 总结</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Peng Liu</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>




        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  

  

  

</body>
</html>
